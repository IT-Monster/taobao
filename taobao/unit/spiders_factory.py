# -*- coding: utf-8 -*-


def create_spider(spider_name, keyword):
    """
    此模块是一个爬虫模板，用于根据搜索关键词自动创建爬虫，爬虫以搜索关键词命名
    :param spider_name: 接收搜索关键词作为爬虫名称
    :return: not return
    """
    class_name = spider_name.capitalize()
    with open('taobao/spiders/' + spider_name + '.py', 'w', encoding='utf-8') as f:
        f.write('# -*- coding: utf-8 -*-\n\n')
        f.write('import scrapy\n')
        f.write('import json\n')
        f.write('from taobao.config.config_operate import get_cookie, get_search_api, get_keyword, get_comment_api, get_crawl_page, get_comment_page\n')
        f.write('from taobao.items import TaobaoItem, GrabRecordsItem\n')
        f.write('from taobao.unit.converter import to_md5\n')
        f.write('from taobao.unit.removal import removal_grab_records\n\n\n')
        f.write('class ' + class_name + 'Spider(scrapy.Spider):\n')
        f.write('   name = "' + spider_name + '"\n')
        f.write('   offset = 0\n')
        f.write('   count = 1\n')
        f.write('   page = 0\n')
        f.write('   comment_count = 0\n\n')
        f.write('   def start_requests(self):\n')
        f.write('       """\n')
        f.write('       重写start_requests()方法，在请求中加入headers\n')
        f.write('       :return: server response\n')
        f.write('       """\n')
        f.write('       cookie = get_cookie()\n')
        f.write('       headers = {\n')
        f.write('           "cookie": cookie,\n')
        f.write('           "referer": get_search_api() + "' + keyword + '"\n')
        f.write('       }\n')
        f.write('       url = get_search_api() + "' + keyword + '&s=0"\n')
        f.write('       yield scrapy.Request(url, headers=headers, callback=self.parse)\n\n')
        f.write('   def parse(self, response):\n')
        f.write('       """\n')
        f.write('       获取商品列表，并获取每件商品的相关数据\n')
        f.write('       :param response: 商品列表\n')
        f.write('       :return: 根据产品的detail_url向服务器发出请求后服务器返回的响应数据\n')
        f.write('           item_id: 商品ID\n')
        f.write('           user_id: 商家ID\n')
        f.write('           title: 商品标题\n')
        f.write('           url: 商品评价API\n')
        f.write('       """\n')
        f.write('       try:\n')
        f.write(r'           data = response.css("script::text").extract()[3].split("\n")[2].strip()[16:].strip(";")' + '\n')
        f.write('           data = json.loads(data)\n')
        f.write('           datas = data["mods"]["itemlist"]["data"]["auctions"]\n')
        f.write('           for data in datas:\n')
        f.write('               item_id = data["nid"]\n')
        f.write('               user_id = data["user_id"]\n')
        f.write('               title = data["raw_title"]\n')
        f.write('               url = "https:" + data["detail_url"]\n')
        f.write('               comment_url = get_comment_api() + item_id + "&sellerId=" + user_id + "&currentPage=1"\n')
        f.write('               cookie = get_cookie()\n')
        f.write('               headers = {\n')
        f.write('                   "cookie": cookie,\n')
        f.write('                   "referer": url\n')
        f.write('               }\n')
        f.write('               page_num = 1\n')
        f.write('               while removal_grab_records(to_md5(comment_url)) == 1:\n')
        f.write('                   page_num += 1\n')
        f.write('                   comment_url = get_comment_api() + item_id + "&sellerId=" + user_id + "&currentPage=" + str(page_num)\n')
        f.write('                   yield scrapy.Request(comment_url, headers=headers, meta={"headers": headers, "item_id": item_id, "user_id": user_id, "title": title}, callback=self.parse_comment, dont_filter=True)\n')
        f.write('                   self.offset += 44\n')
        f.write('                   headers = {\n')
        f.write('                       "cookie": cookie,\n')
        f.write('                       "referer": get_search_api() + get_keyword()\n')
        f.write('                   }\n')
        f.write('                   search_url = get_search_api() + get_keyword() + "&s=" + str(self.offset)\n')
        f.write('                   if self.offset <= get_crawl_page() * 44:\n')
        f.write('                       yield scrapy.Request(search_url, headers=headers, callback=self.parse, dont_filter=True)\n')
        f.write('                       self.count += 1\n')
        f.write('       except json.decoder.JSONDecodeError as e:\n')
        f.write('           print("滑动验证出现了")\n\n')
        f.write('   def parse_comment(self, response):\n')
        f.write('       """\n')
        f.write('       获取商品评论\n')
        f.write('       :param response: 商品评论\n')
        f.write('       :return: item（把获取到的数据传到scrapy的items中，由scrapy的pipelines进行处理）\n')
        f.write('       """\n')
        f.write('       item = TaobaoItem()\n')
        f.write('       grab_records_item = GrabRecordsItem()\n')
        f.write('       headers = response.meta["headers"]\n')
        f.write('       item_id = response.meta["item_id"]\n')
        f.write('       user_id = response.meta["user_id"]\n')
        f.write('       title = response.meta["title"]\n')
        f.write('       grab_url = response.url\n')
        f.write('       try:\n')
        f.write('           data = response.text.split("(")\n')
        f.write('           data1 = ""\n')
        f.write('           for num in range(1, len(data)):\n')
        f.write('               data1 += data[num]\n')
        f.write('           data = data1.strip(")")\n')
        f.write('           data = json.loads(data)\n')
        f.write('           datas = data["rateDetail"]["rateList"]\n')
        f.write('           grab_records_item["grab_id"] = to_md5(grab_url)\n')
        f.write('           grab_records_item["title"] = title\n')
        f.write('           grab_records_item["grab_url"] = grab_url\n')
        f.write('           yield grab_records_item\n')
        f.write('           for data in datas:\n')
        f.write('               rate_content = data["rateContent"]\n')
        f.write('               rate_date = data["rateDate"]\n')
        f.write('               display_user_nick = data["displayUserNick"]\n')
        f.write('               print("商品ID：%s  商家ID：%s  评价人：%s  评价内容：%s  评价时间：%s" % (item_id, user_id, display_user_nick, rate_content, rate_date))\n')
        f.write('               item["comment_id"] = to_md5(item_id + user_id + display_user_nick + rate_content + rate_date)\n')
        f.write('               item["keyword"] = "' + keyword + '"\n')
        f.write('               item["item_id"] = item_id\n')
        f.write('               item["user_id"] = user_id\n')
        f.write('               item["title"] = title\n')
        f.write('               item["comment_user"] = display_user_nick\n')
        f.write('               item["rate_content"] = rate_content\n')
        f.write('               item["rate_date"] = rate_date\n')
        f.write('               self.comment_count += 1\n')
        f.write('               yield item\n')
        f.write('           self.page += 1\n')
        f.write('           if self.page <= get_comment_page():\n')
        f.write('               comment_url = get_comment_api() + item_id + "&sellerId=" + user_id + "&currentPage=" + str(self.page)\n')
        f.write('               if removal_grab_records(to_md5(comment_url)) == 0:\n')
        f.write('                   yield scrapy.Request(comment_url, headers=headers, meta={"headers": headers, "item_id": item_id, "user_id": user_id, "title": title}, callback=self.parse_comment, dont_filter=True)\n')
        f.write('       except json.decoder.JSONDecodeError as e:\n')
        f.write('           print("滑动验证出现了")\n')
